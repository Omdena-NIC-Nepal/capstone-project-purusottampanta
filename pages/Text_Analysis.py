import streamlit as st
import spacy
from spacy import displacy
from googletrans import Translator
import nepali_roman as nr
from transformers import pipeline
import spacy.cli
from spacy.util import is_package

# Initialize translator
translator = Translator()

if not is_package("en_core_web_sm"):
    spacy.cli.download("en_core_web_sm")
    
if not is_package("fr_core_news_sm"):
    spacy.cli.download("fr_core_news_sm")

if not is_package("ja_core_news_sm"):
    spacy.cli.download("ja_core_news_sm")

# Language configurations
LANGUAGES = {
    "English": "en",
    "Nepali": "ne",
    "French": "fr", 
    "Japanese": "ja"
}

TRANSLATIONS = {
    "en": {
        "title": "ðŸŒ Climate Impact Analysis",
        "input_label": "Paste news article:",
        "analyze_btn": "Analyze",
        "severity": "Severity Level",
        "confidence": "Confidence",
        "casualties": "Casualties",
        "impact": "Impact Assessment",
        "entities": "Key Entities",
        "actions": "Impact Actions",
        "technical": "Technical Details",
        "major": "ðŸš¨ Major Disaster",
        "significant": "âš ï¸ Significant Impact",
        "moderate": "âœ… Moderate Impact",
        "deaths": "Confirmed Deaths",
        "missing": "Missing Persons",
    },
    "ne": {
        "title": "ðŸŒ à¤œà¤²à¤µà¤¾à¤¯à¥ à¤ªà¥à¤°à¤­à¤¾à¤µ à¤µà¤¿à¤¶à¥à¤²à¥‡à¤·à¤£",
        "input_label": "à¤¸à¤®à¤¾à¤šà¤¾à¤° à¤Ÿà¤¾à¤à¤¸à¥à¤¨à¥à¤¹à¥‹à¤¸à¥:",
        "analyze_btn": "à¤µà¤¿à¤¶à¥à¤²à¥‡à¤·à¤£ à¤—à¤°à¥à¤¨à¥à¤¹à¥‹à¤¸à¥",
        "severity": "à¤—à¤®à¥à¤­à¥€à¤°à¤¤à¤¾ à¤¸à¥à¤¤à¤°",
        "confidence": "à¤µà¤¿à¤¶à¥à¤µà¤¾à¤¸",
        "casualties": "à¤®à¥ƒà¤¤à¥à¤¯à¥ à¤¸à¤‚à¤–à¥à¤¯à¤¾",
        "impact": "à¤ªà¥à¤°à¤­à¤¾à¤µ à¤®à¥‚à¤²à¥à¤¯à¤¾à¤‚à¤•à¤¨",
        "entities": "à¤®à¥à¤–à¥à¤¯ à¤¸à¤‚à¤¸à¥à¤¥à¤¾à¤¹à¤°à¥‚",
        "actions": "à¤ªà¥à¤°à¤­à¤¾à¤µà¥€ à¤•à¤¾à¤°à¥à¤¯à¤¹à¤°à¥‚",
        "technical": "à¤ªà¥à¤°à¤¾à¤µà¤¿à¤§à¤¿à¤• à¤µà¤¿à¤µà¤°à¤£",
        "major": "ðŸš¨ à¤ à¥‚à¤²à¥‹ à¤ªà¥à¤°à¤•à¥‹à¤ª",
        "significant": "âš ï¸ à¤®à¤¹à¤¤à¥à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤ªà¥à¤°à¤­à¤¾à¤µ",
        "moderate": "âœ… à¤®à¤§à¥à¤¯à¤® à¤ªà¥à¤°à¤­à¤¾à¤µ",
        "deaths": "à¤ªà¥à¤·à¥à¤Ÿà¤¿ à¤­à¤à¤•à¥‹ à¤®à¥ƒà¤¤à¥à¤¯à¥",
        "missing": "à¤¹à¤°à¤¾à¤à¤•à¤¾ à¤µà¥à¤¯à¤•à¥à¤¤à¤¿"
    },
    "fr": {
        "title": "ðŸŒ Analyse d'Impact Climatique",
        "input_label": "Coller l'article de presse:",
        "analyze_btn": "Analyser",
        "severity": "Niveau de GravitÃ©",
        "confidence": "Confiance",
        "casualties": "Victimes",
        "impact": "Ã‰valuation d'Impact",
        "entities": "EntitÃ©s ClÃ©s",
        "actions": "Actions d'Impact",
        "technical": "DÃ©tails Techniques",
        "major": "ðŸš¨ Catastrophe Majeure",
        "significant": "âš ï¸ Impact Important",
        "moderate": "âœ… Impact ModÃ©rÃ©"
    },
    "ja": {
        "title": "ðŸŒ æ°—å€™å½±éŸ¿åˆ†æž",
        "input_label": "ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’è²¼ã‚Šä»˜ã‘:",
        "analyze_btn": "åˆ†æžã™ã‚‹",
        "severity": "æ·±åˆ»åº¦ãƒ¬ãƒ™ãƒ«",
        "confidence": "ä¿¡é ¼åº¦", 
        "casualties": "æ­»è€…æ•°",
        "impact": "å½±éŸ¿è©•ä¾¡",
        "entities": "ä¸»è¦ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£",
        "actions": "å½±éŸ¿ã‚¢ã‚¯ã‚·ãƒ§ãƒ³",
        "technical": "æŠ€è¡“çš„è©³ç´°",
        "major": "ðŸš¨ å¤§è¦æ¨¡ç½å®³",
        "significant": "âš ï¸ é‡å¤§ãªå½±éŸ¿",
        "moderate": "âœ… ä¸­ç­‰åº¦ã®å½±éŸ¿"
    }
}

# Disaster Severity Thresholds
SEVERITY_LEVELS = {
    "Catastrophic": {"min_deaths": 50, "color": "red"},
    "Severe": {"min_deaths": 20, "color": "orange"},
    "Major": {"min_deaths": 5, "color": "yellow"},
    "Moderate": {"min_deaths": 1, "color": "blue"}
}

# Load models
CLIMATE_MODELS = {
    "en": pipeline("text-classification", model="climatebert/environmental-claims"),
    "multilingual": pipeline("text-classification", model="nlptown/bert-base-multilingual-uncased-sentiment")
}

nlp_models = {
    "en": spacy.load("en_core_web_sm"),
    "fr": spacy.load("fr_core_news_sm"),
    "ja": spacy.load("ja_core_news_sm")
}

#translator = Translator()

def analyze_text(text, lang):
    """Advanced multilingual analysis (fixed)"""
    # Translate non-English text
    if lang != "en":
        # translated = translator.translate(text, src=lang, dest='en').text
        translated =  translate_text(text, lang)
    else:
        translated = text
    
    # Use appropriate model
    model = CLIMATE_MODELS["en"] if lang == "en" else CLIMATE_MODELS["multilingual"]
    severity = model(translated)[0]
    
    # Entity analysis
    nlp = nlp_models.get(lang, nlp_models["en"])
    doc = nlp(text)
    entities = [(ent.text, ent.label_) for ent in doc.ents]
    
    # Impact analysis
    impact_verbs = {
        "en": ["trigger", "cause", "destroy", "flood", "displace"],
        "ne": ["à¤Ÿà¥à¤°à¤¿à¤—à¤°", "à¤•à¤¾à¤°à¤£", "à¤¨à¤·à¥à¤Ÿ", "à¤¬à¤¾à¤¢à¥€", "à¤µà¤¿à¤¸à¥à¤¥à¤¾à¤ªà¤¿à¤¤"],
        "fr": ["dÃ©clencher", "causer", "dÃ©truire", "inonder", "dÃ©placer"],
        "ja": ["å¼•ãèµ·ã“ã™", "åŽŸå› ", "ç ´å£Š", "æ´ªæ°´", "ç§»å‹•"]
    }
    verbs = impact_verbs.get(lang, impact_verbs["en"])
    impact_count = sum(1 for token in doc if token.lemma_.lower() in verbs)
    
    # Casualty extraction
    deaths = extract_deaths(text, lang)
    
    
    return {
        "severity": severity['label'],
        "score": severity['score'],
        "entities": entities,
        "impact": impact_count,
        "deaths": deaths,
        "translated": translated
    }

def translate_text(text, src_lang, dest_lang="en"):
    """Synchronous text translation without coroutines"""
    try:
        # Using simple dictionary for trying. While deploying to production appropriate API with Google translate can be used
        translations = {
            "ne": {
                "à¤®à¤°à¥‡": "died",
                "à¤¬à¤¾à¤¢à¥€": "flood",
                "à¤ªà¤¹à¤¿à¤°à¥‹": "landslide"
            },
            "fr": {
                "inondation": "flood",
                "glissement": "landslide"
            },
            "ja": {
                "æ´ªæ°´": "flood",
                "åœ°æ»‘ã‚Š": "landslide"
            }
        }
        
        if src_lang == "en":
            return text
        return ' '.join([translations[src_lang].get(word, word) for word in text.split()])
    
    except Exception as e:
        st.error(f"Translation error: {str(e)}")
        return text

def extract_deaths(text, lang):
    """Multilingual death count extraction (fixed)"""
    nlp = nlp_models.get(lang, nlp_models["en"])
    doc = nlp(text)
    
    # Get cardinal number entities as Span objects
    numbers = [ent for ent in doc.ents if ent.label_ == 'CARDINAL']
    
    death_keywords = {
        "en": ["die", "kill", "fatality"],
        "ne": ["à¤®à¤°à¥‡", "à¤®à¥ƒà¤¤à¥à¤¯à¥", "à¤®à¤¾à¤°à¤¿à¤"],
        "fr": ["mort", "dÃ©cÃ¨s", "tuer"],
        "ja": ["æ­»äº¡", "æ­»è€…", "æ­»äº¡ã—ãŸ"]
    }
    
    # Check proximity between numbers and death keywords
    for token in doc:
        if token.lemma_.lower() in death_keywords.get(lang, death_keywords["en"]):
            # Look for nearby numbers
            window = doc[max(0, token.i-5):token.i+5]
            for ent in numbers:
                if ent.start >= token.i-5 and ent.end <= token.i+5:
                    try:
                        return int(ent.text.replace(",", ""))
                    except:
                        continue
    return 0

def app():
    # Language selection
    lang_name = st.sidebar.selectbox("Choose Language", list(LANGUAGES.keys()))
    lang_code = LANGUAGES[lang_name]
    t = TRANSLATIONS[lang_code]
    
    st.title(t["title"])
    
    text = st.text_area(t["input_label"], height=200)
    
    if st.button(t["analyze_btn"]):
        if text.strip():
            try:
                analysis = analyze_text(text, lang_code)
                
                # Bilingual display
                col1, col2 = st.columns(2)
                
                with col1:  # Selected language
                    st.subheader(t["severity"])
                    st.write(f"{analysis['severity']} ({analysis['score']:.0%})")
                    
                    st.subheader(t["casualties"])
                    st.write(analysis['deaths'] or t["unknown"])
                    
                    st.subheader(t["impact"])
                    if analysis['deaths'] > 50 or analysis['impact'] > 3:
                        st.error(t["major"])
                    elif analysis['deaths'] > 10:
                        st.warning(t["significant"])
                    else:
                        st.success(t["moderate"])
                
                with col2:  # English
                    st.subheader("English Analysis")
                    st.write(f"Severity: {analysis['severity']} ({analysis['score']:.0%})")
                    st.write(f"Casualties: {analysis['deaths']}")
                    st.write(f"Impact Verbs: {analysis['impact']}")
                
                # Entity visualization
                st.subheader(t["entities"])
                if analysis['entities']:
                    html = displacy.render(nlp_models.get(lang_code, nlp_models["en"])(text), 
                                      style="ent")
                    st.components.v1.html(html, height=300)
                else:
                    st.info(t["no_entities"])
                
                # Technical details
                with st.expander(t["technical"]):
                    st.json(analysis)
            
            except Exception as e:
                st.error(f"Error: {str(e)}")
        else:
            st.warning(t["no_text"])

# Add remaining translations
for lang in TRANSLATIONS:
    TRANSLATIONS[lang].update({
        "unknown": "Unknown",
        "no_entities": "No entities detected",
        "no_text": "Please input text"
    })

if __name__ == "__main__":
    app()